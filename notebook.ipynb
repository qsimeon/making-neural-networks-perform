{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0-ml1sj4no-z4yxnc",
      "metadata": {},
      "source": [
        "# Neural Networks for Iterative Matrix Algorithms\n",
        "\n",
        "This notebook demonstrates how neural networks can be designed to perform iterative versions of essential matrix algorithms, specifically:\n",
        "\n",
        "- Matrix Inversion using iterative gradient descent\n",
        "- Principal Component Analysis (PCA) using Oja's rule\n",
        "- Comparison with standard numerical methods\n",
        "\n",
        "These neural network approaches provide biologically-plausible and online learning alternatives to traditional batch matrix algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-1-ml1sj4np-2muma1",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import eigh\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-2-ml1sj4np-cvg467",
      "metadata": {},
      "source": [
        "## 1. Neural Network for Matrix Inversion\n",
        "\n",
        "We implement an iterative neural network approach to compute the inverse of a matrix A. The network uses the update rule: W(t+1) = W(t) + η(I - AW(t))W(t)^T, where W converges to A^(-1).\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-3-ml1sj4np-8ydgbc",
      "metadata": {},
      "source": [
        "class NeuralMatrixInversion:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.history = []\n",
        "    \n",
        "    def fit(self, A, n_iterations=1000, tol=1e-6):\n",
        "        \"\"\"\n",
        "        Iteratively compute the inverse of matrix A\n",
        "        \"\"\"\n",
        "        n = A.shape[0]\n",
        "        # Initialize W with small random values\n",
        "        W = np.random.randn(n, n) * 0.01\n",
        "        I = np.eye(n)\n",
        "        \n",
        "        for i in range(n_iterations):\n",
        "            # Compute error: I - AW\n",
        "            error = I - A @ W\n",
        "            \n",
        "            # Update rule: W = W + η * error * W^T\n",
        "            W = W + self.learning_rate * (error @ W + W @ error.T)\n",
        "            \n",
        "            # Track convergence\n",
        "            error_norm = np.linalg.norm(error)\n",
        "            self.history.append(error_norm)\n",
        "            \n",
        "            if error_norm < tol:\n",
        "                print(f\"Converged at iteration {i}\")\n",
        "                break\n",
        "        \n",
        "        return W\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-4-ml1sj4np-ewifn8",
      "metadata": {},
      "source": [
        "## 2. Test Matrix Inversion\n",
        "\n",
        "Let's test our neural network approach on a random symmetric positive definite matrix and compare with NumPy's built-in inverse.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-5-ml1sj4np-mg3r6q",
      "metadata": {},
      "source": [
        "# Create a symmetric positive definite matrix\n",
        "n = 5\n",
        "A_temp = np.random.randn(n, n)\n",
        "A = A_temp @ A_temp.T + np.eye(n) * 0.1  # Ensure positive definite\n",
        "\n",
        "# Neural network inversion\n",
        "nn_inv = NeuralMatrixInversion(learning_rate=0.05)\n",
        "W_neural = nn_inv.fit(A, n_iterations=2000)\n",
        "\n",
        "# Standard inversion\n",
        "A_inv_true = np.linalg.inv(A)\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nNeural Network Inverse (first 3x3 block):\")\n",
        "print(W_neural[:3, :3])\n",
        "print(\"\\nTrue Inverse (first 3x3 block):\")\n",
        "print(A_inv_true[:3, :3])\n",
        "print(\"\\nFrobenius norm of difference:\", np.linalg.norm(W_neural - A_inv_true))\n",
        "\n",
        "# Verify: A * A_inv should be identity\n",
        "product = A @ W_neural\n",
        "print(\"\\nA * W_neural (should be identity):\")\n",
        "print(product[:3, :3])\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-6-ml1sj4np-7s6ub9",
      "metadata": {},
      "source": [
        "## 3. Visualize Convergence\n",
        "\n",
        "Plot the error norm over iterations to visualize how the neural network converges to the matrix inverse.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-7-ml1sj4np-rj1cie",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(nn_inv.history, linewidth=2)\n",
        "plt.xlabel('Iteration', fontsize=12)\n",
        "plt.ylabel('Error Norm ||I - AW||', fontsize=12)\n",
        "plt.title('Convergence of Neural Matrix Inversion', fontsize=14)\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-8-ml1sj4np-dx521r",
      "metadata": {},
      "source": [
        "## 4. Oja's Rule for PCA\n",
        "\n",
        "Oja's rule is a biologically-inspired neural learning rule that extracts the principal component (first eigenvector) of the data covariance matrix. The update rule is: w(t+1) = w(t) + η(x(x^T w) - (w^T x)^2 w), which converges to the first principal component.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-9-ml1sj4np-0apl6m",
      "metadata": {},
      "source": [
        "class OjasPCA:\n",
        "    def __init__(self, n_components=1, learning_rate=0.01):\n",
        "        self.n_components = n_components\n",
        "        self.learning_rate = learning_rate\n",
        "        self.components_ = None\n",
        "        self.history = []\n",
        "    \n",
        "    def fit(self, X, n_iterations=1000):\n",
        "        \"\"\"\n",
        "        Extract principal components using Oja's rule\n",
        "        X: data matrix (n_samples x n_features)\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Center the data\n",
        "        X_centered = X - np.mean(X, axis=0)\n",
        "        \n",
        "        # Initialize weight vectors\n",
        "        W = np.random.randn(n_features, self.n_components) * 0.01\n",
        "        \n",
        "        for iteration in range(n_iterations):\n",
        "            # Randomly sample a data point\n",
        "            idx = np.random.randint(0, n_samples)\n",
        "            x = X_centered[idx].reshape(-1, 1)\n",
        "            \n",
        "            # Oja's rule for multiple components\n",
        "            y = W.T @ x  # Output\n",
        "            \n",
        "            # Update: W = W + η(xy^T - yy^T W)\n",
        "            dW = self.learning_rate * (x @ y.T - W @ (y @ y.T))\n",
        "            W = W + dW\n",
        "            \n",
        "            # Normalize to prevent unbounded growth\n",
        "            for j in range(self.n_components):\n",
        "                W[:, j] = W[:, j] / (np.linalg.norm(W[:, j]) + 1e-8)\n",
        "            \n",
        "            # Track convergence every 100 iterations\n",
        "            if iteration % 100 == 0:\n",
        "                self.history.append(np.linalg.norm(dW))\n",
        "        \n",
        "        self.components_ = W.T\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Project data onto principal components\"\"\"\n",
        "        X_centered = X - np.mean(X, axis=0)\n",
        "        return X_centered @ self.components_.T\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-10-ml1sj4np-rm8bmh",
      "metadata": {},
      "source": [
        "## 5. Generate Test Data for PCA\n",
        "\n",
        "Create synthetic data with known principal components to test Oja's rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-11-ml1sj4np-bb2q5g",
      "metadata": {},
      "source": [
        "# Generate synthetic data with clear principal components\n",
        "n_samples = 500\n",
        "n_features = 3\n",
        "\n",
        "# Create data with variance along specific directions\n",
        "t = np.linspace(0, 4*np.pi, n_samples)\n",
        "X_data = np.zeros((n_samples, n_features))\n",
        "X_data[:, 0] = 3 * np.cos(t) + np.random.randn(n_samples) * 0.1\n",
        "X_data[:, 1] = 2 * np.sin(t) + np.random.randn(n_samples) * 0.1\n",
        "X_data[:, 2] = 0.5 * np.random.randn(n_samples)\n",
        "\n",
        "# Rotate the data\n",
        "rotation_matrix = np.array([[0.8, -0.6, 0], [0.6, 0.8, 0], [0, 0, 1]])\n",
        "X_data = X_data @ rotation_matrix.T\n",
        "\n",
        "print(f\"Data shape: {X_data.shape}\")\n",
        "print(f\"Data mean: {np.mean(X_data, axis=0)}\")\n",
        "print(f\"Data std: {np.std(X_data, axis=0)}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-12-ml1sj4np-jruvyn",
      "metadata": {},
      "source": [
        "## 6. Apply Oja's Rule and Compare with Standard PCA\n",
        "\n",
        "Train the neural network using Oja's rule and compare the extracted principal components with those from standard eigenvalue decomposition.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-13-ml1sj4np-gle3yh",
      "metadata": {},
      "source": [
        "# Apply Oja's rule\n",
        "oja_pca = OjasPCA(n_components=2, learning_rate=0.001)\n",
        "oja_pca.fit(X_data, n_iterations=50000)\n",
        "\n",
        "# Standard PCA using eigenvalue decomposition\n",
        "X_centered = X_data - np.mean(X_data, axis=0)\n",
        "cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n",
        "eigenvalues, eigenvectors = eigh(cov_matrix)\n",
        "\n",
        "# Sort by eigenvalues (descending)\n",
        "idx = eigenvalues.argsort()[::-1]\n",
        "eigenvalues = eigenvalues[idx]\n",
        "eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "print(\"\\nOja's Rule - First Principal Component:\")\n",
        "print(oja_pca.components_[0])\n",
        "print(\"\\nStandard PCA - First Principal Component:\")\n",
        "print(eigenvectors[:, 0])\n",
        "\n",
        "print(\"\\nOja's Rule - Second Principal Component:\")\n",
        "print(oja_pca.components_[1])\n",
        "print(\"\\nStandard PCA - Second Principal Component:\")\n",
        "print(eigenvectors[:, 1])\n",
        "\n",
        "# Compute alignment (absolute dot product, since direction can be flipped)\n",
        "alignment_1 = abs(np.dot(oja_pca.components_[0], eigenvectors[:, 0]))\n",
        "alignment_2 = abs(np.dot(oja_pca.components_[1], eigenvectors[:, 1]))\n",
        "print(f\"\\nAlignment of 1st component: {alignment_1:.4f} (1.0 = perfect)\")\n",
        "print(f\"Alignment of 2nd component: {alignment_2:.4f} (1.0 = perfect)\")\n",
        "\n",
        "print(f\"\\nEigenvalues (variance explained): {eigenvalues}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-14-ml1sj4np-k0ifvu",
      "metadata": {},
      "source": [
        "## 7. Visualize PCA Results\n",
        "\n",
        "Project the data onto the principal components found by both methods and visualize the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-15-ml1sj4np-6vigyc",
      "metadata": {},
      "source": [
        "# Project data onto principal components\n",
        "X_oja = oja_pca.transform(X_data)\n",
        "X_standard = X_centered @ eigenvectors[:, :2]\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Oja's rule projection\n",
        "axes[0].scatter(X_oja[:, 0], X_oja[:, 1], alpha=0.5, s=20)\n",
        "axes[0].set_xlabel('First Principal Component', fontsize=11)\n",
        "axes[0].set_ylabel('Second Principal Component', fontsize=11)\n",
        "axes[0].set_title(\"Oja's Rule PCA Projection\", fontsize=13)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axis('equal')\n",
        "\n",
        "# Standard PCA projection\n",
        "axes[1].scatter(X_standard[:, 0], X_standard[:, 1], alpha=0.5, s=20, color='orange')\n",
        "axes[1].set_xlabel('First Principal Component', fontsize=11)\n",
        "axes[1].set_ylabel('Second Principal Component', fontsize=11)\n",
        "axes[1].set_title('Standard PCA Projection', fontsize=13)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-16-ml1sj4np-dhxtvy",
      "metadata": {},
      "source": [
        "## 8. Visualize Learning Dynamics\n",
        "\n",
        "Plot the convergence of Oja's rule over training iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-17-ml1sj4np-d5lopu",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(oja_pca.history, linewidth=2, color='green')\n",
        "plt.xlabel('Iteration (x100)', fontsize=12)\n",
        "plt.ylabel('Weight Update Norm', fontsize=12)\n",
        "plt.title(\"Convergence of Oja's Rule\", fontsize=14)\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-18-ml1sj4np-5c5cwe",
      "metadata": {},
      "source": [
        "## 9. Performance Comparison\n",
        "\n",
        "Compare the computational characteristics of neural network approaches versus standard methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-19-ml1sj4np-7nlkwu",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "# Time comparison for matrix inversion\n",
        "sizes = [5, 10, 20, 30]\n",
        "neural_times = []\n",
        "standard_times = []\n",
        "\n",
        "for size in sizes:\n",
        "    # Create test matrix\n",
        "    A_temp = np.random.randn(size, size)\n",
        "    A_test = A_temp @ A_temp.T + np.eye(size) * 0.1\n",
        "    \n",
        "    # Neural network approach\n",
        "    start = time.time()\n",
        "    nn_inv_test = NeuralMatrixInversion(learning_rate=0.05)\n",
        "    nn_inv_test.fit(A_test, n_iterations=1000)\n",
        "    neural_times.append(time.time() - start)\n",
        "    \n",
        "    # Standard approach\n",
        "    start = time.time()\n",
        "    _ = np.linalg.inv(A_test)\n",
        "    standard_times.append(time.time() - start)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(sizes, neural_times, 'o-', label='Neural Network (1000 iter)', linewidth=2, markersize=8)\n",
        "plt.plot(sizes, standard_times, 's-', label='Standard np.linalg.inv', linewidth=2, markersize=8)\n",
        "plt.xlabel('Matrix Size', fontsize=12)\n",
        "plt.ylabel('Time (seconds)', fontsize=12)\n",
        "plt.title('Computation Time Comparison: Matrix Inversion', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Observations:\")\n",
        "print(\"- Standard methods are faster for batch processing\")\n",
        "print(\"- Neural approaches enable online/incremental learning\")\n",
        "print(\"- Neural methods are biologically plausible and parallelizable\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-20-ml1sj4np-wpksht",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how neural networks can perform iterative versions of essential matrix algorithms:\n",
        "\n",
        "1. Matrix Inversion: We implemented a neural network that iteratively converges to the matrix inverse using gradient-based updates. The method successfully approximated the inverse with high accuracy.\n",
        "2. PCA via Oja's Rule: We implemented Oja's biologically-inspired learning rule that extracts principal components through online learning. The extracted components closely matched those from standard eigenvalue decomposition.\n",
        "3. Trade-offs: While standard numerical methods are faster for batch processing, neural approaches offer advantages in online learning, biological plausibility, and potential for hardware parallelization.\n",
        "\n",
        "These neural network approaches are particularly valuable in scenarios requiring incremental learning, real-time adaptation, or neuromorphic computing implementations.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}